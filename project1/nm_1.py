# -*- coding: utf-8 -*-
"""NM_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17aXOv6DNVjN0z5L_0VhkHLO0LpgwS96Q
"""

!pip install librosa pandas numpy matplotlib seaborn scikit-learn torch torchaudio transformers soundfile
!pip install speechrecognition pyaudio powerbiclient



!pip install librosa pandas

import librosa

# Correct path to an extracted .flac file
audio_path = "C:\\Users\\Administrator\\Downloads\\dev-clean\\LibriSpeech\\dev-clean\\84\\121123\\84-121123-0000.flac"

# Load the audio
y, sr = librosa.load(audio_path, sr=16000)

print(f"Audio Shape: {y.shape}, Sampling Rate: {sr}")

import librosa.display
import matplotlib.pyplot as plt

# Extract Mel spectrogram
mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)

# Convert power spectrogram to dB (log scale)
mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)

# Plot it
plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_spectrogram_db, sr=sr, x_axis='time', y_axis='mel')
plt.colorbar(format='%+2.0f dB')
plt.title('Mel Spectrogram')
plt.tight_layout()
plt.show()

# Extract MFCCs
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # typically 13 coefficients

# Plot MFCCs
plt.figure(figsize=(10, 4))
librosa.display.specshow(mfccs, sr=sr, x_axis='time')
plt.colorbar()
plt.title('MFCCs')
plt.tight_layout()
plt.show()

import os
import librosa
import numpy as np
import pandas as pd

# Path to your extracted audio folder
audio_folder = "C:\\Users\\Administrator\\Downloads\\dev-clean\\LibriSpeech\\dev-clean"

# List to store extracted features
data = []

for root, dirs, files in os.walk(audio_folder):
    for file in files:
        if file.endswith('.flac'):
            file_path = os.path.join(root, file)

            try:
                y, sr = librosa.load(file_path, sr=16000)

                # Extract MFCCs
                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
                mfccs_mean = np.mean(mfccs, axis=1)  # Take mean of each MFCC across time

                # Extract Mel Spectrogram
                mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
                mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
                mel_mean = np.mean(mel_spectrogram_db, axis=1)

                # Store
                feature_row = {
                    'file_path': file_path,
                    **{f'mfcc_{i}': mfcc for i, mfcc in enumerate(mfccs_mean)},
                    **{f'mel_{i}': mel for i, mel in enumerate(mel_mean)}
                }
                data.append(feature_row)

            except Exception as e:
                print(f"Error processing {file_path}: {e}")

df_features = pd.DataFrame(data)
print(df_features.shape)
df_features.head()

df_features.to_csv('audio_features.csv', index=False)

import librosa
import numpy as np
import pandas as pd
import os

# Path to your .wav file
audio_path = "C:/Users/Administrator/Downloads/sample.wav"  # Using forward slashes


# Load the audio file
y, sr = librosa.load(audio_path, sr=16000)

# Extract MFCC (Mel Frequency Cepstral Coefficients)
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

# Transpose the MFCC matrix to have each row as an observation (time step)
mfccs = mfccs.T

# Convert the MFCCs into a DataFrame for easy export to CSV
df_mfcc = pd.DataFrame(mfccs)

# Optionally, you can add additional columns like time steps or other features if needed
# Save the DataFrame to CSV
csv_path = "audio_features.csv"  # Output CSV file path
df_mfcc.to_csv(csv_path, index=False)

# Print out the first few rows of the extracted features
print(df_mfcc.head())

# Display the waveform
plt.figure(figsize=(10, 6))
plt.plot(y)
plt.title('Raw Audio Waveform')
plt.show()

pip install sox ffmpeg

import torchaudio
import torch  # Importing torch
import numpy as np
from IPython.display import Audio

# Load the audio file (replace with your actual file path)
audio_path = "C:\\Users\\Administrator\\Downloads\\rainy-day-in-town-with-birds-singing-194011.mp3"
waveform, sample_rate = torchaudio.load(audio_path)

# Function to add noise to the waveform
def add_noise(audio, noise_level=0.005):
    noise = np.random.randn(*audio.shape)  # Generate Gaussian noise
    noisy_audio = audio + noise_level * noise
    return np.clip(noisy_audio, -1.0, 1.0)  # Clip values to be within the range [-1, 1]

# Add noise to the waveform
noisy_waveform = add_noise(waveform[0].numpy())  # Assuming it's a mono file (waveform[0])

# Save the noisy waveform to a new file
torchaudio.save("noisy_rainy_day_sample.wav", torch.tensor([noisy_waveform]), sample_rate)


# Display the noisy audio file
from IPython.display import Audio
display(Audio("noisy_rainy_day_sample.wav"))

import soundfile as sf
data, samplerate = sf.read("noisy_rainy_day_sample.wav")
sf.write("output.wav", data, samplerate)

!pip install pydub
!pip install nltk
!apt-get install ffmpeg

!pip install pydub nltk ffmpeg-python

!pip install gTTS

!pip install gTTS pydub nltk ffmpeg-python

import subprocess

# Check FFmpeg version
try:
    result = subprocess.run(['ffmpeg', '-version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    print(result.stdout)
except FileNotFoundError:
    print("FFmpeg is not installed or not found in the system PATH.")

pip install gTTS pydub nltk

from gtts import gTTS
from pydub import AudioSegment
from pydub.playback import play
import nltk

# Download necessary NLTK packages (if not already done)
nltk.download('punkt')
nltk.download('stopwords')

# Sample dataset text (replace this with your actual dataset text)
text = """Clerk banking house transported embezzlement though grave doubts guilt entertained..."""  # Add full dataset text here

# Convert text to audio using gTTS (Google Text-to-Speech)
tts = gTTS(text=text, lang='en')

# Save the generated speech to an MP3 file
tts.save('dataset_audio.mp3')  # The audio file will be saved

# Load and play the audio using pydub
audio = AudioSegment.from_mp3('dataset_audio.mp3')
play(audio)  # You will hear the audio output here

from gtts import gTTS
from IPython.display import Audio

# Sample dataset text (replace this with your actual dataset text)
text = """The bank clerk was transported for embezzlement, though grave doubts as to his guilt were entertained.
When the muster-bell rang, the gang broke up, and Rufus Dawes went in his silent way to his separate cell.
The notable change in the custom and disposition of the new convict was observed."""  # Add full dataset text here

# Convert text to audio using gTTS (Google Text-to-Speech)
tts = gTTS(text=text, lang='en')

# Save the generated speech to an MP3 file
audio_path = 'dataset_audio.mp3'
tts.save(audio_path)

# Display audio player in Jupyter to play the audio
Audio(audio_path)

import speech_recognition as sr
from pydub import AudioSegment

# Convert mp3 to wav (because SpeechRecognition works well with wav format)
audio_path = 'dataset_audio.mp3'
audio = AudioSegment.from_mp3(audio_path)
audio.export("dataset_audio.wav", format="wav")  # Save it as a wav file

# Initialize recognizer
recognizer = sr.Recognizer()

# Load the audio file (wav format)
with sr.AudioFile("dataset_audio.wav") as source:
    # Adjust for ambient noise and record audio
    recognizer.adjust_for_ambient_noise(source)
    audio_data = recognizer.record(source)

# Recognize speech using Google's speech recognition API
try:
    text_from_audio = recognizer.recognize_google(audio_data)
    print("Transcribed Text from Audio:", text_from_audio)
except sr.UnknownValueError:
    print("Google Speech Recognition could not understand the audio")
except sr.RequestError as e:
    print(f"Could not request results from Google Speech Recognition service; {e}")



# Step 1: Import Required Libraries
import os
import tarfile
import librosa
import numpy as np
import matplotlib.pyplot as plt
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch
from IPython.display import Audio

# Step 2: Extract TAR.GZ File
def extract_tar_gz(file_path, extract_path='librispeech_data'):
    if not os.path.exists(extract_path):
        os.makedirs(extract_path)
    with tarfile.open(file_path, 'r:gz') as tar:
        tar.extractall(path=extract_path)
    print(f"Extracted to: {extract_path}")
    return extract_path

# Step 3: Load and Preprocess Audio
def load_audio(file_path, target_sr=16000):
    audio, sr = librosa.load(file_path, sr=target_sr)
    audio = audio / np.max(np.abs(audio))  # Normalize
    return audio, sr

# Step 4: Visualize Audio
def plot_waveform(audio, sr):
    plt.figure(figsize=(10, 4))
    plt.plot(np.linspace(0, len(audio)/sr, len(audio)), audio)
    plt.title("Waveform")
    plt.xlabel("Time (s)")
    plt.ylabel("Amplitude")
    plt.show()

# Step 5: ASR Model Class
class ASRModel:
    def __init__(self):
        self.processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
        self.model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
        self.model.eval()

    def transcribe(self, audio):
        inputs = self.processor(audio, return_tensors="pt", sampling_rate=16000)
        with torch.no_grad():
            logits = self.model(inputs.input_values).logits
        predicted_ids = torch.argmax(logits, dim=-1)
        return self.processor.batch_decode(predicted_ids)[0]

# Step 6: Main Execution
def run_asr_pipeline(tar_gz_path):
    extracted_path = extract_tar_gz(tar_gz_path)

    # Search for .flac files (LibriSpeech format)
    flac_files = []
    for root, _, files in os.walk(extracted_path):
        for file in files:
            if file.endswith(".flac") or file.endswith(".wav"):
                flac_files.append(os.path.join(root, file))

    if not flac_files:
        print("No audio files found.")
        return

    model = ASRModel()

    for file_path in flac_files[:5]:  # Limit to 5 samples for demo
        print(f"\nüîπ Processing File: {os.path.basename(file_path)}")
        audio, sr = load_audio(file_path)
        plot_waveform(audio, sr)
        display(Audio(audio, rate=sr))  # Playback
        transcription = model.transcribe(audio)
        print(f"üìù Transcription: {transcription}")

# Example usage:
# run_asr_pipeline("dev-clean.tar.gz")  # <-- Replace with actual uploaded filename

run_asr_pipeline("dev-clean.tar.gz")